#!/usr/bin/env python3
"""
å°† OpenRCA metric å’Œ trace æ•°æ®å¯¼å…¥åˆ° SigNoz
ä½¿ç”¨ CSV ä¸­çš„æ—¶é—´æˆ³ä½œä¸ºä¸ŠæŠ¥æ•°æ®çš„æ—¶é—´
é¡ºåºï¼šå…ˆå¯¼å…¥ metricï¼Œå†å¯¼å…¥ trace
"""

import csv
import glob
from pathlib import Path
from typing import List, Dict, Optional
from collections import defaultdict
from opentelemetry import trace, metrics
from opentelemetry.sdk.trace import ReadableSpan
from opentelemetry.sdk.trace.export import SpanExportResult
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.exporter.otlp.proto.grpc.metric_exporter import OTLPMetricExporter
from opentelemetry.sdk.resources import Resource
from opentelemetry.trace import SpanContext, TraceFlags, Status, StatusCode
from opentelemetry.util.types import AttributeValue
from opentelemetry.sdk.trace import InstrumentationScope
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader, MetricExportResult, MetricsData
from opentelemetry.sdk.metrics.view import View, ExplicitBucketHistogramAggregation
from opentelemetry.sdk.metrics._internal.point import (
    ResourceMetrics, ScopeMetrics, Metric, NumberDataPoint, HistogramDataPoint,
    Sum, Gauge, Histogram as HistogramData
)
from opentelemetry.sdk.util.instrumentation import InstrumentationScope
import argparse
from datetime import datetime, timezone, timedelta
import time
from tqdm import tqdm
import logging
import traceback

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


# ============================================================================
# é…ç½®ç®¡ç†ç±»
# ============================================================================

class Config:
    """ç»Ÿä¸€ç®¡ç†é…ç½®å‚æ•°"""
    DEFAULT_BATCH_SIZE = 1000
    DEFAULT_EXPORTER_WAIT_TIME = 2  # ç§’
    DEFAULT_SERVICE_NAME = "openrca-bank"
    DEFAULT_SERVICE_VERSION = "1.0.0"
    
    def __init__(
        self,
        signoz_endpoint: str,
        batch_size: int = DEFAULT_BATCH_SIZE,
        service_name: str = DEFAULT_SERVICE_NAME,
        time_offset_ms: int = 0,
        exporter_wait_time: int = DEFAULT_EXPORTER_WAIT_TIME
    ):
        self.signoz_endpoint = signoz_endpoint
        self.batch_size = batch_size
        self.service_name = service_name
        self.time_offset_ms = time_offset_ms
        self.exporter_wait_time = exporter_wait_time


# ============================================================================
# Resource ç®¡ç†å™¨
# ============================================================================

class ResourceManager:
    """ç®¡ç† OpenTelemetry Resource çš„åˆ›å»ºå’Œç¼“å­˜"""
    
    def __init__(self, service_version: str = "1.0.0"):
        self._cache: Dict[str, Resource] = {}
        self.service_version = service_version
    
    def get_resource(self, service_name: str) -> Resource:
        """è·å–æˆ–åˆ›å»º Resourceï¼Œä½¿ç”¨ç¼“å­˜é¿å…é‡å¤åˆ›å»º"""
        if service_name not in self._cache:
            self._cache[service_name] = Resource.create({
                "service.name": service_name,
                "service.version": self.service_version,
            })
        return self._cache[service_name]
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self._cache.clear()


# ============================================================================
# Exporter ç®¡ç†å™¨
# ============================================================================

class ExporterManager:
    """ç»Ÿä¸€ç®¡ç† OpenTelemetry Exporters çš„åˆ›å»ºå’Œå…³é—­"""
    
    def __init__(self, config: Config):
        self.config = config
        self._trace_exporter: Optional[OTLPSpanExporter] = None
        self._metric_exporter: Optional[OTLPMetricExporter] = None
    
    def _normalize_endpoint(self, endpoint: str) -> str:
        """è§„èŒƒåŒ– endpoint URL"""
        if not endpoint.startswith('http'):
            return f"http://{endpoint}"
        return endpoint
    
    def _create_exporter(self, exporter_class, endpoint: str, **kwargs):
        """é€šç”¨çš„ exporter åˆ›å»ºæ–¹æ³•ï¼Œå¸¦é”™è¯¯å¤„ç†"""
        endpoint_url = self._normalize_endpoint(endpoint)
        try:
            return exporter_class(endpoint=endpoint_url, insecure=True, **kwargs)
        except Exception:
            # å¦‚æœå¤±è´¥ï¼Œå°è¯•ä¸ä½¿ç”¨ http:// å‰ç¼€
            return exporter_class(endpoint=endpoint, insecure=True, **kwargs)
    
    def get_trace_exporter(self) -> OTLPSpanExporter:
        """è·å–æˆ–åˆ›å»º Trace Exporter"""
        if self._trace_exporter is None:
            self._trace_exporter = self._create_exporter(
                OTLPSpanExporter,
                self.config.signoz_endpoint
            )
        return self._trace_exporter
    
    def get_metric_exporter(self) -> OTLPMetricExporter:
        """è·å–æˆ–åˆ›å»º Metric Exporter"""
        if self._metric_exporter is None:
            self._metric_exporter = self._create_exporter(
                OTLPMetricExporter,
                self.config.signoz_endpoint
            )
        return self._metric_exporter
    
    def shutdown_all(self):
        """å…³é—­æ‰€æœ‰ exporters"""
        if self._trace_exporter:
            try:
                time.sleep(self.config.exporter_wait_time)
                self._trace_exporter.shutdown()
                logger.info("Trace Exporter å·²å…³é—­")
            except Exception as e:
                logger.warning(f"å…³é—­ Trace Exporter æ—¶å‡ºé”™: {e}")
        
        if self._metric_exporter:
            try:
                time.sleep(self.config.exporter_wait_time)
                self._metric_exporter.shutdown()
                logger.info("Metric Exporter å·²å…³é—­")
            except Exception as e:
                logger.warning(f"å…³é—­ Metric Exporter æ—¶å‡ºé”™: {e}")


# ============================================================================
# å·¥å…·å‡½æ•°
# ============================================================================

def convert_to_trace_id(trace_id_str: str) -> int:
    """å°† trace_id å­—ç¬¦ä¸²è½¬æ¢ä¸º OpenTelemetry æ ¼å¼çš„ trace_id (128ä½æ•´æ•°)"""
    # ä½¿ç”¨ hash() æ–¹æ³•ï¼ˆä¸ä¹‹å‰èƒ½æ­£å¸¸å·¥ä½œçš„ç‰ˆæœ¬ä¸€è‡´ï¼‰
    # æ³¨æ„ï¼šè™½ç„¶ hash() å¯èƒ½å›  hash randomization åœ¨ä¸åŒè¿è¡Œé—´äº§ç”Ÿä¸åŒå€¼ï¼Œ
    # ä½†åœ¨åŒä¸€è¿›ç¨‹å†…æ˜¯ç¨³å®šçš„ï¼Œè¿™å¯¹äºå¯¼å…¥å†å²æ•°æ®æ˜¯å¯ä»¥æ¥å—çš„
    hash_val = abs(hash(trace_id_str))
    # trace_id æ˜¯ 128 ä½ï¼Œæ‰€ä»¥å–æ¨¡ 2^128
    trace_id = hash_val % (2**128)
    # OpenTelemetry è¦æ±‚ trace_id ä¸èƒ½ä¸ºå…¨é›¶
    if trace_id == 0:
        trace_id = 1
    return trace_id


def convert_to_span_id(span_id_str: str) -> int:
    """å°† span_id å­—ç¬¦ä¸²è½¬æ¢ä¸º OpenTelemetry æ ¼å¼çš„ span_id (64ä½æ•´æ•°)"""
    # ä½¿ç”¨ hash() æ–¹æ³•ï¼ˆä¸ä¹‹å‰èƒ½æ­£å¸¸å·¥ä½œçš„ç‰ˆæœ¬ä¸€è‡´ï¼‰
    hash_val = abs(hash(span_id_str))
    # span_id æ˜¯ 64 ä½ï¼Œæ‰€ä»¥å–æ¨¡ 2^64
    span_id = hash_val % (2**64)
    # OpenTelemetry è¦æ±‚ span_id ä¸èƒ½ä¸ºå…¨é›¶
    if span_id == 0:
        span_id = 1
    return span_id


def parse_timestamp(timestamp_str: str, time_offset_ms: int = 0, is_seconds: bool = False) -> int:
    """å°†æ—¶é—´æˆ³å­—ç¬¦ä¸²è½¬æ¢ä¸ºçº³ç§’ï¼ˆOpenTelemetry ä½¿ç”¨çº³ç§’ï¼‰
    
    Args:
        timestamp_str: CSV ä¸­çš„æ—¶é—´æˆ³
        time_offset_ms: æ—¶é—´åç§»é‡ï¼ˆæ¯«ç§’ï¼‰ï¼Œç”¨äºå°†å†å²æ—¶é—´æ˜ å°„åˆ°ç›®æ ‡æ—¶é—´
        is_seconds: å¦‚æœä¸ºTrueï¼Œæ—¶é—´æˆ³æ˜¯ç§’ï¼›å¦‚æœä¸ºFalseï¼Œæ—¶é—´æˆ³æ˜¯æ¯«ç§’
    """
    try:
        if is_seconds:
            # CSV ä¸­çš„æ—¶é—´æˆ³æ˜¯ç§’ï¼ˆmetricæ–‡ä»¶ï¼‰
            timestamp_s = int(float(timestamp_str))
            # è½¬æ¢ä¸ºæ¯«ç§’
            timestamp_ms = timestamp_s * 1000
        else:
            # CSV ä¸­çš„æ—¶é—´æˆ³æ˜¯æ¯«ç§’ï¼ˆtraceæ–‡ä»¶ï¼‰
            timestamp_ms = int(float(timestamp_str))
        # åº”ç”¨æ—¶é—´åç§»
        timestamp_ms = timestamp_ms + time_offset_ms
        # è½¬æ¢ä¸ºçº³ç§’
        return timestamp_ms * 1_000_000
    except (ValueError, TypeError):
        raise ValueError(f"æ— æ³•è§£ææ—¶é—´æˆ³: {timestamp_str}")


class SimpleSpan(ReadableSpan):
    """ç®€å•çš„ Span å®ç°ï¼Œç”¨äºä» CSV æ•°æ®åˆ›å»º"""
    
    def __init__(
        self,
        name: str,
        context: SpanContext,
        parent: Optional[SpanContext],
        resource: Resource,
        attributes: Dict[str, AttributeValue],
        start_time_ns: int,
        end_time_ns: int,
    ):
        self._name = name
        self._context = context
        self._parent = parent
        self._resource = resource
        self._attributes = attributes
        self._start_time = start_time_ns
        self._end_time = end_time_ns
        self._events = []
        self._links = []
        self._kind = trace.SpanKind.INTERNAL
        self._status = Status(StatusCode.OK)
        # åˆ›å»ºé»˜è®¤çš„ InstrumentationScope
        self._instrumentation_scope = InstrumentationScope(
            name=__name__,
            version=None
        )
    
    @property
    def name(self) -> str:
        return self._name
    
    @property
    def context(self) -> SpanContext:
        return self._context
    
    @property
    def parent(self) -> Optional[SpanContext]:
        return self._parent
    
    @property
    def resource(self) -> Resource:
        return self._resource
    
    @property
    def attributes(self) -> Dict[str, AttributeValue]:
        return self._attributes
    
    @property
    def events(self) -> List:
        return self._events
    
    @property
    def links(self) -> List:
        return self._links
    
    @property
    def kind(self) -> trace.SpanKind:
        return self._kind
    
    @property
    def status(self) -> Status:
        return self._status
    
    @property
    def start_time(self) -> int:
        return self._start_time
    
    @property
    def end_time(self) -> int:
        return self._end_time
    
    @property
    def instrumentation_scope(self) -> InstrumentationScope:
        return self._instrumentation_scope


def create_span_data_from_row(
    row: Dict[str, str], 
    time_offset_ms: int = 0,
    resource_manager: Optional[ResourceManager] = None
) -> Optional[ReadableSpan]:
    """ä» CSV è¡Œåˆ›å»º OpenTelemetry SpanData
    
    Args:
        row: CSV è¡Œæ•°æ®
        time_offset_ms: æ—¶é—´åç§»é‡ï¼ˆæ¯«ç§’ï¼‰ï¼Œç”¨äºå°†å†å²æ—¶é—´æ˜ å°„åˆ°ç›®æ ‡æ—¶é—´
        resource_manager: Resource ç®¡ç†å™¨ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨å…¨å±€å®ä¾‹
    """
    if resource_manager is None:
        resource_manager = _resource_manager
    
    try:
        # è§£ææ—¶é—´æˆ³ï¼ˆåº”ç”¨æ—¶é—´åç§»ï¼‰
        start_time_ns = parse_timestamp(row['timestamp'], time_offset_ms)
        
        # è§£ææŒç»­æ—¶é—´ï¼ˆæ¯«ç§’è½¬çº³ç§’ï¼‰
        duration_ms = int(float(row.get('duration', 0)))
        duration_ns = duration_ms * 1_000_000
        end_time_ns = start_time_ns + duration_ns if duration_ns > 0 else start_time_ns + 1
        
        # è·å– span ä¿¡æ¯
        span_id_str = row.get('span_id', '')
        trace_id_str = row.get('trace_id', '')
        parent_id_str = row.get('parent_id', '')
        cmdb_id = row.get('cmdb_id', 'unknown')
        
        # æ ¹æ® cmdb_id åˆ›å»ºå¯¹åº”çš„ resource
        resource = resource_manager.get_resource(cmdb_id)
        
        # è½¬æ¢ trace_id å’Œ span_id
        trace_id = convert_to_trace_id(trace_id_str)
        span_id = convert_to_span_id(span_id_str)
        
        # åˆ›å»º SpanContext
        span_context = SpanContext(
            trace_id=trace_id,
            span_id=span_id,
            is_remote=False,
            trace_flags=TraceFlags(0x01)  # SAMPLED
        )
        
        # åˆ›å»ºçˆ¶ SpanContextï¼ˆå¦‚æœæœ‰ï¼‰
        parent_context = None
        if parent_id_str and parent_id_str.strip():
            parent_span_id = convert_to_span_id(parent_id_str)
            parent_context = SpanContext(
                trace_id=trace_id,  # åŒä¸€ trace ä½¿ç”¨ç›¸åŒçš„ trace_id
                span_id=parent_span_id,
                is_remote=False,
                trace_flags=TraceFlags(0x01)
            )
        
        # åˆ›å»ºå±æ€§
        attributes: Dict[str, AttributeValue] = {
            "cmdb_id": cmdb_id,
            "span_id": span_id_str,
            "trace_id": trace_id_str,
            "duration_ms": duration_ms,
        }
        if parent_id_str:
            attributes["parent_id"] = parent_id_str
        
        # åˆ›å»º SimpleSpan
        span = SimpleSpan(
            name=cmdb_id,
            context=span_context,
            parent=parent_context,
            resource=resource,
            attributes=attributes,
            start_time_ns=start_time_ns,
            end_time_ns=end_time_ns,
        )
        
        return span
            
    except Exception as e:
        print(f"åˆ›å»º span æ—¶å‡ºé”™: {e}, è¡Œæ•°æ®: {row}")
        return None




# å…¨å±€ Resource ç®¡ç†å™¨å®ä¾‹ï¼ˆå‘åå…¼å®¹ï¼‰
_resource_manager = ResourceManager()

def create_resource_for_service(service_name: str) -> Resource:
    """æ ¹æ®æœåŠ¡åç§°åˆ›å»º Resourceï¼ˆå‘åå…¼å®¹å‡½æ•°ï¼‰
    ä½¿ç”¨ ResourceManager è¿›è¡Œç¼“å­˜ç®¡ç†
    """
    return _resource_manager.get_resource(service_name)


def process_metric_app_file(
    file_path: str,
    exporter: OTLPMetricExporter,
    batch_size: int,
    time_range_info: Optional[Dict] = None,
    time_offset_ms: int = 0,
    show_progress: bool = True,
    resource_manager: Optional[ResourceManager] = None
) -> int:
    """å¤„ç† metric_app.csv æ–‡ä»¶ï¼Œä½¿ç”¨è‡ªå®šä¹‰æ—¶é—´æˆ³
    æ ¼å¼: timestamp,rr,sr,cnt,mrt,tc
    - timestamp: æ—¶é—´æˆ³ï¼ˆç§’ï¼‰
    - rr: è¯·æ±‚ç‡ (Request Rate)
    - sr: æˆåŠŸç‡ (Success Rate)
    - cnt: è®¡æ•° (Count)
    - mrt: å¹³å‡å“åº”æ—¶é—´ (Mean Response Time)
    - tc: æœåŠ¡åç§° (Service Name)
    
    æŒ‰ (timestamp, tc) åˆ†ç»„ï¼Œæ¯ä¸ªåˆ†ç»„åˆ›å»ºä¸€ä¸ª ResourceMetrics
    """
    if resource_manager is None:
        resource_manager = _resource_manager
    
    count = 0
    batch_rows = []  # å­˜å‚¨å¾…å¤„ç†çš„åŸå§‹è¡Œæ•°æ®
    
    # åˆ›å»ºInstrumentationScope
    instrumentation_scope = InstrumentationScope(
        name=__name__,
        version=None
    )
    
    def create_resource_metrics_from_group(group_key, group_rows):
        """ä»åˆ†ç»„æ•°æ®åˆ›å»º ResourceMetrics"""
        service_name = group_key
                        
        # æ ¹æ® service_name åˆ›å»ºå¯¹åº”çš„ resource
        resource = resource_manager.get_resource(service_name)
        
        # åˆ›å»ºå±æ€§
        attributes = {"service.name": service_name}
        
        # æ”¶é›†è¯¥åˆ†ç»„çš„æ‰€æœ‰æ•°æ®ç‚¹
        rr_data_points = []
        sr_data_points = []
        cnt_data_points = []
        mrt_data_points = []
        
        for row in group_rows:
            try:
                rr = float(row.get('rr', 0))
                sr = float(row.get('sr', 0))
                cnt = float(row.get('cnt', 0))
                mrt = float(row.get('mrt', 0))
                
                timestamp_s = int(float(row.get('timestamp', 0)))
                if timestamp_s <= 0:
                    continue
                
                timestamp_ms = timestamp_s * 1000 + time_offset_ms
                timestamp_ns = timestamp_ms * 1_000_000
                
                # è·Ÿè¸ªæ—¶é—´èŒƒå›´
                if time_range_info is not None:
                    if time_range_info.get('min') is None or timestamp_ms < time_range_info['min']:
                        time_range_info['min'] = timestamp_ms
                    if time_range_info.get('max') is None or timestamp_ms > time_range_info['max']:
                        time_range_info['max'] = timestamp_ms
                        
                # app_request_rate (rr) - Counter
                if rr > 0:
                    rr_data_points.append(NumberDataPoint(
                        attributes=attributes,
                        start_time_unix_nano=timestamp_ns,
                        time_unix_nano=timestamp_ns,
                        value=rr
                    ))
                
                # app_success_rate (sr) - Gauge
                if sr >= 0:
                    sr_data_points.append(NumberDataPoint(
                        attributes=attributes,
                        start_time_unix_nano=timestamp_ns,
                        time_unix_nano=timestamp_ns,
                        value=sr
                    ))
                
                # app_request_count (cnt) - Counter
                if cnt > 0:
                    cnt_data_points.append(NumberDataPoint(
                        attributes=attributes,
                        start_time_unix_nano=timestamp_ns,
                        time_unix_nano=timestamp_ns,
                        value=cnt
                    ))
                
                # app_response_time (mrt) - Histogram
                if mrt > 0:
                    explicit_bounds = [0.0, 5.0, 10.0, 25.0, 50.0, 75.0, 100.0, 250.0, 500.0, 750.0, 1000.0, 2500.0, 5000.0, 7500.0, 10000.0]
                    bucket_counts = [0] * (len(explicit_bounds) + 1)
                    bucket_idx = len(explicit_bounds)
                    for i, bound in enumerate(explicit_bounds):
                        if mrt <= bound:
                            bucket_idx = i
                            break
                    bucket_counts[bucket_idx] = 1
                    
                    mrt_data_points.append(HistogramDataPoint(
                        attributes=attributes,
                        start_time_unix_nano=timestamp_ns,
                        time_unix_nano=timestamp_ns,
                        count=1,
                        sum=mrt,
                        bucket_counts=bucket_counts,
                        explicit_bounds=explicit_bounds,
                        min=mrt,
                        max=mrt
                    ))
            except (ValueError, TypeError):
                continue
        
        # åˆ›å»º metrics
        metrics_list = []
        
        if rr_data_points:
            metrics_list.append(Metric(
                name="app_request_rate",
                description="Application request rate",
                unit="1",
                data=Sum(
                    data_points=rr_data_points,
                    aggregation_temporality=1,
                    is_monotonic=True
                )
            ))
        
        if sr_data_points:
            metrics_list.append(Metric(
                name="app_success_rate",
                description="Application success rate",
                unit="1",
                data=Gauge(data_points=sr_data_points)
            ))
        
        if cnt_data_points:
            metrics_list.append(Metric(
                name="app_request_count",
                description="Application request count",
                unit="1",
                data=Sum(
                    data_points=cnt_data_points,
                    aggregation_temporality=1,
                    is_monotonic=True
                )
            ))
        
        if mrt_data_points:
            metrics_list.append(Metric(
                name="app_response_time",
                description="Application mean response time",
                unit="ms",
                data=HistogramData(
                    data_points=mrt_data_points,
                    aggregation_temporality=1
                )
            ))
        
        if not metrics_list:
            return None
        
        # åˆ›å»º ResourceMetrics
        scope_metrics = ScopeMetrics(
            scope=instrumentation_scope,
            metrics=metrics_list,
            schema_url=""
        )
        return ResourceMetrics(
            resource=resource,
            scope_metrics=[scope_metrics],
            schema_url=""
        )
    
    def process_batch():
        """å¤„ç†å½“å‰æ‰¹æ¬¡çš„æ•°æ®ï¼šåˆ†ç»„å¹¶åˆ›å»º ResourceMetrics"""
        if not batch_rows:
            return []
        
        # æŒ‰ (timestamp, tc) åˆ†ç»„
        groups = defaultdict(list)
        for row in batch_rows:
            service_name = row.get('tc', 'unknown')
            try:                
                group_key = service_name
                groups[group_key].append(row)
            except (ValueError, TypeError):
                continue
        
        # ä¸ºæ¯ä¸ªåˆ†ç»„åˆ›å»º ResourceMetrics
        resource_metrics_list = []
        for group_key, group_rows in groups.items():
            resource_metrics = create_resource_metrics_from_group(group_key, group_rows)
            if resource_metrics:
                resource_metrics_list.append(resource_metrics)
        
        return resource_metrics_list
    
    try:
        total_lines = count_file_lines(file_path) if show_progress else 0
        
        with open(file_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            
            pbar = None
            if show_progress and total_lines > 0:
                pbar = tqdm(
                    total=total_lines,
                    desc="    å¤„ç†ä¸­",
                    unit="è¡Œ",
                    unit_scale=True,
                    leave=False,
                    ncols=100,
                    mininterval=0.5,
                    bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'
                )
            
            try:
                for row in reader:
                    try:
                        batch_rows.append(row)
                        count += 1
                        
                        if pbar:
                            pbar.update(1)
                        
                        # å½“æ‰¹æ¬¡è¾¾åˆ°æŒ‡å®šå¤§å°æ—¶ï¼Œå¤„ç†å¹¶å‘é€
                        if len(batch_rows) >= batch_size:
                            resource_metrics_list = process_batch()
                            
                            if resource_metrics_list:
                                try:
                                    metrics_data = MetricsData(resource_metrics=resource_metrics_list)
                                    result = exporter.export(metrics_data)
                                    if result != MetricExportResult.SUCCESS:
                                        if pbar:
                                            pbar.write(f"    âš ï¸  è­¦å‘Š: æ‰¹æ¬¡å¯¼å‡ºè¿”å›: {result}")
                                except Exception as e:
                                    if pbar:
                                        pbar.write(f"    âŒ é”™è¯¯: å¯¼å‡ºæ‰¹æ¬¡æ—¶å‡ºé”™: {e}")
                                    logger.exception("å¯¼å‡ºæ‰¹æ¬¡æ—¶å‡ºé”™")
                            
                            batch_rows = []
                            
                    except Exception as e:
                        if pbar:
                            pbar.write(f"    âš ï¸  å¤„ç†è¡Œæ—¶å‡ºé”™: {e}")
                        continue
                        
            finally:
                if pbar:
                    pbar.close()
        
        # å¤„ç†å¹¶å‘é€å‰©ä½™çš„æ‰¹æ¬¡
        if batch_rows:
            resource_metrics_list = process_batch()
            if resource_metrics_list:
                try:
                    metrics_data = MetricsData(resource_metrics=resource_metrics_list)
                    result = exporter.export(metrics_data)
                    if result != MetricExportResult.SUCCESS:
                        if show_progress:
                            print(f"    âš ï¸  è­¦å‘Š: æœ€åæ‰¹æ¬¡å¯¼å‡ºè¿”å›: {result}")
                except Exception as e:
                    if show_progress:
                        print(f"    âŒ é”™è¯¯: å¯¼å‡ºæœ€åæ‰¹æ¬¡æ—¶å‡ºé”™: {e}")
                    logger.exception("å¯¼å‡ºæœ€åæ‰¹æ¬¡æ—¶å‡ºé”™")
        
        return count
        
    except Exception as e:
        if show_progress:
            print(f"    âŒ å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        logger.exception("å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™")
        return 0


def extract_kpi_category(kpi_name: str) -> str:
    """ä»kpi_nameä¸­æå–å¤§ç±»
    
    æ”¯æŒçš„åˆ†ç±»ï¼š
    - Mysql: MySQLæ•°æ®åº“ç›¸å…³æŒ‡æ ‡
    - OSLinux: æ“ä½œç³»ç»ŸLinuxç›¸å…³æŒ‡æ ‡
    - redis: Redisç›¸å…³æŒ‡æ ‡
    - Container: Dockerå®¹å™¨ç›¸å…³æŒ‡æ ‡
    - Tomcat: Tomcatåº”ç”¨æœåŠ¡å™¨ç›¸å…³æŒ‡æ ‡
    - JVM: Javaè™šæ‹Ÿæœºç›¸å…³æŒ‡æ ‡
    
    Returns:
        å¤§ç±»åç§°ï¼ˆå°å†™ï¼‰ï¼Œä¾‹å¦‚ï¼šmysql, oslinux, redis, container, tomcat, jvm
    """
    if not kpi_name:
        return 'other'
    
    # ç§»é™¤å¼•å·
    kpi_name = kpi_name.strip('"')
    
    # æå–ç¬¬ä¸€ä¸ª'-'ä¹‹å‰çš„éƒ¨åˆ†ä½œä¸ºå¤§ç±»
    if '-' in kpi_name:
        category = kpi_name.split('-')[0].strip()
    else:
        return 'other'
    
    # æ ‡å‡†åŒ–ç±»åˆ«åç§°
    category_lower = category.lower()
    
    # æ˜ å°„åˆ°æ ‡å‡†ç±»åˆ«åç§°
    category_mapping = {
        'mysql': 'mysql',
        'oslinux': 'oslinux',
        'redis': 'redis',
        'container': 'container',
        'tomcat': 'tomcat',
        'jvm': 'jvm',
    }
    
    return category_mapping.get(category_lower, 'other')


def process_metric_container_file(
    file_path: str,
    exporter: OTLPMetricExporter,
    batch_size: int,
    time_range_info: Optional[Dict] = None,
    time_offset_ms: int = 0,
    show_progress: bool = True,
    resource_manager: Optional[ResourceManager] = None
) -> int:
    """å¤„ç† metric_container.csv æ–‡ä»¶ï¼Œä½¿ç”¨è‡ªå®šä¹‰æ—¶é—´æˆ³
    æ ¼å¼: timestamp,cmdb_id,kpi_name,value
    - timestamp: æ—¶é—´æˆ³ï¼ˆç§’ï¼‰
    - cmdb_id: ç»„ä»¶ID
    - kpi_name: KPIæŒ‡æ ‡åç§°
    - value: æŒ‡æ ‡å€¼
    
    æŒ‰ (timestamp, cmdb_id) åˆ†ç»„ï¼Œæ¯ä¸ªåˆ†ç»„åˆ›å»ºä¸€ä¸ª ResourceMetrics
    é»˜è®¤æŒ‰å¤§ç±»åˆ†ç»„åˆ›å»ºmetricï¼ˆä¾‹å¦‚ï¼šcontainer_mysql_metric, container_oslinux_metricç­‰ï¼‰ã€‚
    å°†349ä¸ªmetricå‡å°‘åˆ°6ä¸ªï¼ˆæ¯ä¸ªå¤§ç±»ä¸€ä¸ªï¼‰ã€‚
    """
    if resource_manager is None:
        resource_manager = _resource_manager
    
    count = 0  # è¯»å–çš„CSVè¡Œæ•°
    total_data_points = 0  # å®é™…åˆ›å»ºçš„metricæ•°æ®ç‚¹æ€»æ•°
    total_batches_exported = 0  # å¯¼å‡ºçš„æ‰¹æ¬¡æ€»æ•°
    total_resource_metrics = 0  # åˆ›å»ºçš„ResourceMetricsæ€»æ•°
    batch_rows = []  # å­˜å‚¨å¾…å¤„ç†çš„åŸå§‹è¡Œæ•°æ®
    
    # åˆ›å»ºInstrumentationScope
    instrumentation_scope = InstrumentationScope(
        name=__name__,
        version=None
    )
    
    def create_resource_metrics_from_group(group_key, group_rows):
        """ä»åˆ†ç»„æ•°æ®åˆ›å»º ResourceMetrics"""
        cmdb_id = group_key
        
        # æ ¹æ® cmdb_id åˆ›å»ºå¯¹åº”çš„ resource
        resource = resource_manager.get_resource(cmdb_id)
        
        # æŒ‰å¤§ç±»åˆ†ç»„æ”¶é›†æ•°æ®ç‚¹
        # category -> metric_name -> data_points
        category_metrics = defaultdict(lambda: defaultdict(list))
        
        for row in group_rows:
            try:
                timestamp_s = int(float(row.get('timestamp', 0)))
                if timestamp_s <= 0:
                    continue
                timestamp_ms = timestamp_s * 1000 + time_offset_ms
                timestamp_ns = timestamp_ms * 1_000_000
                
                # è·Ÿè¸ªæ—¶é—´èŒƒå›´
                if time_range_info is not None:
                    if time_range_info.get('min') is None or timestamp_ms < time_range_info['min']:
                        time_range_info['min'] = timestamp_ms
                    if time_range_info.get('max') is None or timestamp_ms > time_range_info['max']:
                        time_range_info['max'] = timestamp_ms
                        
                kpi_name = row.get('kpi_name', '')
                if not kpi_name:
                    continue
                
                value = float(row.get('value', 0))
                
                # æŒ‰å¤§ç±»ç¡®å®šmetricåç§°
                category = extract_kpi_category(kpi_name)
                metric_name = f"container_{category}_metric"
                
                # åˆ›å»ºå±æ€§
                attributes = {
                    "cmdb_id": cmdb_id,
                    "kpi_name": kpi_name,
                }
                
                # åˆ›å»ºæ•°æ®ç‚¹
                data_point = NumberDataPoint(
                    attributes=attributes,
                    start_time_unix_nano=timestamp_ns,
                    time_unix_nano=timestamp_ns,
                    value=value
                )
                
                category_metrics[category][metric_name].append(data_point)
            except (ValueError, TypeError):
                continue
        
        if not category_metrics:
            return None
        
        # ä¸ºæ¯ä¸ªå¤§ç±»åˆ›å»º Metric
        metrics_list = []
        for category, metric_dict in category_metrics.items():
            for metric_name, data_points in metric_dict.items():
                gauge_data = Gauge(data_points=data_points)
                metric = Metric(
                    name=metric_name,
                    description=f"Container metrics for {category} category. Use kpi_name label to filter specific metrics.",
                    unit="1",
                    data=gauge_data
                )
                metrics_list.append(metric)
        
        if not metrics_list:
            return None
        
        # åˆ›å»º ResourceMetrics
        scope_metrics = ScopeMetrics(
            scope=instrumentation_scope,
            metrics=metrics_list,
            schema_url=""
        )
        return ResourceMetrics(
            resource=resource,
            scope_metrics=[scope_metrics],
            schema_url=""
        )
    
    def process_batch():
        """å¤„ç†å½“å‰æ‰¹æ¬¡çš„æ•°æ®ï¼šåˆ†ç»„å¹¶åˆ›å»º ResourceMetrics"""
        if not batch_rows:
            return []
        
        # æŒ‰ (timestamp, cmdb_id) åˆ†ç»„
        groups = defaultdict(list)
        for row in batch_rows:
            try:
                cmdb_id = row.get('cmdb_id', 'unknown')
                
                group_key = cmdb_id
                groups[group_key].append(row)
            except (ValueError, TypeError):
                continue
        
        # ä¸ºæ¯ä¸ªåˆ†ç»„åˆ›å»º ResourceMetrics
        resource_metrics_list = []
        for group_key, group_rows in groups.items():
            resource_metrics = create_resource_metrics_from_group(group_key, group_rows)
            if resource_metrics:
                resource_metrics_list.append(resource_metrics)
        
        return resource_metrics_list
    
    try:
        total_lines = count_file_lines(file_path) if show_progress else 0
        
        with open(file_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            
            pbar = None
            if show_progress and total_lines > 0:
                pbar = tqdm(
                    total=total_lines,
                    desc="    å¤„ç†ä¸­",
                    unit="è¡Œ",
                    unit_scale=True,
                    leave=False,
                    ncols=100,
                    mininterval=0.5,
                    bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'
                )
            
            try:
                for row in reader:
                    try:
                        batch_rows.append(row)
                        count += 1
                        
                        if pbar:
                            pbar.update(1)
                        
                        # å½“æ‰¹æ¬¡è¾¾åˆ°æŒ‡å®šå¤§å°æ—¶ï¼Œå¤„ç†å¹¶å‘é€
                        if len(batch_rows) >= batch_size:
                            resource_metrics_list = process_batch()
                            
                            if resource_metrics_list:
                                try:
                                    metrics_data = MetricsData(resource_metrics=resource_metrics_list)
                                    result = exporter.export(metrics_data)
                                    total_batches_exported += 1
                                    total_resource_metrics += len(resource_metrics_list)
                                    # ç»Ÿè®¡æ•°æ®ç‚¹
                                    for rm in resource_metrics_list:
                                        for sm in rm.scope_metrics:
                                            for metric in sm.metrics:
                                                if hasattr(metric.data, 'data_points'):
                                                    total_data_points += len(metric.data.data_points)
                                    
                                    if result != MetricExportResult.SUCCESS:
                                        if pbar:
                                            pbar.write(f"    âš ï¸  è­¦å‘Š: æ‰¹æ¬¡ #{total_batches_exported} å¯¼å‡ºè¿”å›: {result}")
                                except Exception as e:
                                    if pbar:
                                        pbar.write(f"    âŒ é”™è¯¯: æ‰¹æ¬¡ #{total_batches_exported + 1} å¯¼å‡ºæ—¶å‡ºé”™: {e}")
                                    logger.exception("å¯¼å‡ºæ‰¹æ¬¡æ—¶å‡ºé”™")
                            
                            batch_rows = []
                            
                    except Exception as e:
                        if pbar:
                            pbar.write(f"    âš ï¸  å¤„ç†è¡Œæ—¶å‡ºé”™: {e}")
                        continue
                
            finally:
                if pbar:
                    pbar.close()
        
        # å¤„ç†å¹¶å‘é€å‰©ä½™çš„æ‰¹æ¬¡
        if batch_rows:
            resource_metrics_list = process_batch()
            if resource_metrics_list:
                try:
                    metrics_data = MetricsData(resource_metrics=resource_metrics_list)
                    result = exporter.export(metrics_data)
                    total_batches_exported += 1
                    total_resource_metrics += len(resource_metrics_list)
                    # ç»Ÿè®¡æ•°æ®ç‚¹
                    for rm in resource_metrics_list:
                        for sm in rm.scope_metrics:
                            for metric in sm.metrics:
                                if hasattr(metric.data, 'data_points'):
                                    total_data_points += len(metric.data.data_points)
                    
                    if result != MetricExportResult.SUCCESS:
                        if show_progress:
                            print(f"    âš ï¸  è­¦å‘Š: æœ€åæ‰¹æ¬¡å¯¼å‡ºè¿”å›: {result}")
                except Exception as e:
                    if show_progress:
                        print(f"    âŒ é”™è¯¯: å¯¼å‡ºæœ€åæ‰¹æ¬¡æ—¶å‡ºé”™: {e}")
                    logger.exception("å¯¼å‡ºæœ€åæ‰¹æ¬¡æ—¶å‡ºé”™")
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        if show_progress:
            print()
            print("=" * 60)
            print("ğŸ“Š Container Metric ç»Ÿè®¡ä¿¡æ¯:")
            print("=" * 60)
            print(f"  ğŸ“„ è¯»å–çš„CSVè¡Œæ•°: {count:,}")
            print(f"  ğŸ“Š åˆ›å»ºçš„Metricæ•°æ®ç‚¹æ€»æ•°: {total_data_points:,}")
            print(f"  ğŸ“¦ åˆ›å»ºçš„ResourceMetricsæ€»æ•°: {total_resource_metrics:,}")
            print(f"  ğŸ“¤ å¯¼å‡ºçš„æ‰¹æ¬¡æ€»æ•°: {total_batches_exported:,}")
            if total_batches_exported > 0:
                avg_batch_size = total_resource_metrics / total_batches_exported
                print(f"  ğŸ“ˆ å¹³å‡æ¯æ‰¹æ¬¡ResourceMetricsæ•°: {avg_batch_size:.1f}")
            print("=" * 60)
            print()
        
        return count
        
    except Exception as e:
        if show_progress:
            print(f"    âŒ å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        logger.exception("å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™")
        return 0


# ============================================================================
# æ‰¹å¤„ç†å·¥å…·ç±»
# ============================================================================

class BatchProcessor:
    """é€šç”¨çš„æ‰¹å¤„ç†å·¥å…·ç±»ï¼Œç”¨äºå¤„ç† CSV æ•°æ®å¹¶æ‰¹é‡å¯¼å‡º"""
    
    def __init__(self, batch_size: int, exporter, show_progress: bool = True):
        self.batch_size = batch_size
        self.exporter = exporter
        self.show_progress = show_progress
    
    def export_batch(self, batch_data, batch_type: str = "data"):
        """å¯¼å‡ºæ‰¹æ¬¡æ•°æ®ï¼Œç»Ÿä¸€çš„é”™è¯¯å¤„ç†"""
        if not batch_data:
            return True
        
        try:
            if batch_type == "metrics":
                result = self.exporter.export(batch_data)
                return result == MetricExportResult.SUCCESS
            elif batch_type == "spans":
                result = self.exporter.export(batch_data)
                return result == SpanExportResult.SUCCESS
            else:
                logger.warning(f"æœªçŸ¥çš„æ‰¹æ¬¡ç±»å‹: {batch_type}")
                return False
        except Exception as e:
            logger.error(f"å¯¼å‡ºæ‰¹æ¬¡æ—¶å‡ºé”™: {e}", exc_info=True)
            return False


def count_file_lines(file_path: str) -> int:
    """å¿«é€Ÿç»Ÿè®¡æ–‡ä»¶è¡Œæ•°ï¼ˆä¸åŒ…æ‹¬è¡¨å¤´ï¼‰"""
    try:
        # ä½¿ç”¨äºŒè¿›åˆ¶æ¨¡å¼è¯»å–ï¼Œé€Ÿåº¦æ›´å¿«
        with open(file_path, 'rb') as f:
            # è·³è¿‡è¡¨å¤´è¡Œ
            next(f)
            # ç»Ÿè®¡å‰©ä½™è¡Œæ•°ï¼ˆä½¿ç”¨ç¼“å†²åŒºè¯»å–ï¼‰
            count = 0
            buf_size = 1024 * 1024  # 1MB ç¼“å†²åŒº
            while True:
                buf = f.read(buf_size)
                if not buf:
                    break
                count += buf.count(b'\n')
            return count
    except Exception:
        # å¦‚æœäºŒè¿›åˆ¶æ¨¡å¼å¤±è´¥ï¼Œå›é€€åˆ°æ–‡æœ¬æ¨¡å¼
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                next(f)  # è·³è¿‡è¡¨å¤´
                return sum(1 for _ in f)
        except Exception:
            return 0


def process_file_streaming(
    file_path: str,
    exporter: OTLPSpanExporter,
    batch_size: int,
    time_range_info: Optional[Dict] = None,
    time_offset_ms: int = 0,
    show_progress: bool = True,
    resource_manager: Optional[ResourceManager] = None
) -> int:
    """æµå¼å¤„ç†å•ä¸ªæ–‡ä»¶ï¼Œè¾¹è¯»è¾¹å‘é€"""
    count = 0
    batch = []
    file_count = 0
    min_timestamp = None
    max_timestamp = None
    
    try:
        # ç»Ÿè®¡æ–‡ä»¶æ€»è¡Œæ•°ç”¨äºè¿›åº¦æ¡
        total_lines = count_file_lines(file_path) if show_progress else 0
        
        with open(file_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            
            # åˆ›å»ºè¿›åº¦æ¡
            pbar = None
            if show_progress and total_lines > 0:
                pbar = tqdm(
                    total=total_lines,
                    desc="    å¤„ç†ä¸­",
                    unit="è¡Œ",
                    unit_scale=True,
                    leave=False,
                    ncols=100,
                    mininterval=0.5,  # æœ€å°æ›´æ–°é—´éš”0.5ç§’
                    bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'
                )
            
            try:
                for row in reader:
                    # è·Ÿè¸ªæ—¶é—´èŒƒå›´
                    try:
                        timestamp_ms = int(float(row.get('timestamp', 0)))
                        if timestamp_ms > 0:
                            if min_timestamp is None or timestamp_ms < min_timestamp:
                                min_timestamp = timestamp_ms
                            if max_timestamp is None or timestamp_ms > max_timestamp:
                                max_timestamp = timestamp_ms
                    except (ValueError, TypeError):
                        pass
                    
                    span = create_span_data_from_row(row, time_offset_ms, resource_manager)
                    if span:
                        batch.append(span)
                        file_count += 1
                        count += 1
                        
                        # æ›´æ–°è¿›åº¦æ¡
                        if pbar:
                            pbar.update(1)
                        
                        # å½“æ‰¹æ¬¡è¾¾åˆ°æŒ‡å®šå¤§å°æ—¶ï¼Œç«‹å³å‘é€
                        if len(batch) >= batch_size:
                            try:
                                result = exporter.export(batch)
                                if result != SpanExportResult.SUCCESS:
                                    if pbar:
                                        pbar.write(f"    âš ï¸  è­¦å‘Š: æ‰¹æ¬¡å¯¼å‡ºè¿”å›: {result}")
                            except Exception as e:
                                if pbar:
                                    pbar.write(f"    âŒ é”™è¯¯: å¯¼å‡ºæ‰¹æ¬¡æ—¶å‡ºé”™: {e}")
                                logger.exception("å¯¼å‡ºæ‰¹æ¬¡æ—¶å‡ºé”™")
                            finally:
                                batch = []
                
                # å‘é€å‰©ä½™çš„ spans
                if batch:
                    try:
                        result = exporter.export(batch)
                        if result != SpanExportResult.SUCCESS:
                            if pbar:
                                pbar.write(f"    âš ï¸  è­¦å‘Š: æœ€åæ‰¹æ¬¡å¯¼å‡ºè¿”å›: {result}")
                    except Exception as e:
                        if pbar:
                            pbar.write(f"    âŒ é”™è¯¯: å¯¼å‡ºæœ€åæ‰¹æ¬¡æ—¶å‡ºé”™: {e}")
                        logger.exception("å¯¼å‡ºæœ€åæ‰¹æ¬¡æ—¶å‡ºé”™")
            finally:
                if pbar:
                    pbar.close()
        
        # æ›´æ–°å…¨å±€æ—¶é—´èŒƒå›´ä¿¡æ¯
        if time_range_info is not None:
            if min_timestamp is not None:
                if time_range_info.get('min') is None or min_timestamp < time_range_info['min']:
                    time_range_info['min'] = min_timestamp
            if max_timestamp is not None:
                if time_range_info.get('max') is None or max_timestamp > time_range_info['max']:
                    time_range_info['max'] = max_timestamp
        
        return file_count
        
    except Exception as e:
        if show_progress:
            print(f"    âŒ å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        logger.exception("å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™")
        return 0


def main():
    parser = argparse.ArgumentParser(description='å°† OpenRCA metric å’Œ trace æ•°æ®å¯¼å…¥åˆ° SigNoz')
    parser.add_argument(
        '--signoz-endpoint',
        type=str,
        default='localhost:4317',
        help='SigNoz OTLP gRPC ç«¯ç‚¹ (é»˜è®¤: localhost:4317)'
    )
    parser.add_argument(
        '--data-dir',
        type=str,
        default=None,
        help='æ•°æ®ç›®å½•è·¯å¾„ (å¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™æ ¹æ® --source-date è‡ªåŠ¨æ„å»ºï¼Œä¾‹å¦‚: datasets/OpenRCA/Bank/telemetry/2021_03_04)'
    )
    parser.add_argument(
        '--service-name',
        type=str,
        default=Config.DEFAULT_SERVICE_NAME,
        help=f'æœåŠ¡åç§° (é»˜è®¤: {Config.DEFAULT_SERVICE_NAME})'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=Config.DEFAULT_BATCH_SIZE,
        help=f'æ‰¹å¤„ç†å¤§å° (é»˜è®¤: {Config.DEFAULT_BATCH_SIZE})'
    )
    parser.add_argument(
        '--source-date',
        type=str,
        required=True,
        metavar='YYYY-MM-DD',
        help='æºæ—¥æœŸï¼šdatasets ä¸­çš„å†å²æ—¥æœŸï¼Œæ ¼å¼ YYYY-MM-DD (ä¾‹å¦‚: 2021-03-04)'
    )
    parser.add_argument(
        '--target-date',
        type=str,
        required=True,
        metavar='YYYY-MM-DD',
        help='ç›®æ ‡æ—¥æœŸï¼šæ˜ å°„åˆ°çš„æ—¥æœŸï¼ˆå¿…é¡»åœ¨å½“å‰æ—¶é—´15å¤©å†…ï¼‰ï¼Œæ ¼å¼ YYYY-MM-DD (ä¾‹å¦‚: 2024-12-19)'
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®å¯¹è±¡
    config = Config(
        signoz_endpoint=args.signoz_endpoint,
        batch_size=args.batch_size,
        service_name=args.service_name
    )
    
    # åˆ›å»ºèµ„æºç®¡ç†å™¨å’Œ exporter ç®¡ç†å™¨
    resource_manager = ResourceManager(service_version=Config.DEFAULT_SERVICE_VERSION)
    exporter_manager = ExporterManager(config)
    
    print("=" * 60)
    print("ğŸš€ OpenRCA Metric & Trace æ•°æ®å¯¼å…¥å·¥å…·")
    print("=" * 60)
    print(f"  ğŸ“ SigNoz ç«¯ç‚¹: {args.signoz_endpoint}")
    print(f"  ğŸ·ï¸  æœåŠ¡åç§°: {args.service_name} (ä»…ç”¨äºexporteråˆå§‹åŒ–)")
    print(f"  ğŸ“¦ æ‰¹å¤„ç†å¤§å°: {args.batch_size}")
    print(f"  ğŸ’¡ Resource è¯´æ˜: å°†æ ¹æ®æ•°æ®æ–‡ä»¶ä¸­çš„ tc/cmdb_id åŠ¨æ€åˆ›å»º")
    print("=" * 60)
    print()
    
    # ç¡®å®šæ•°æ®ç›®å½•è·¯å¾„
    if args.data_dir is None:
        # å¦‚æœæ²¡æœ‰æŒ‡å®š data-dirï¼Œæ ¹æ® source-date è‡ªåŠ¨æ„å»ºè·¯å¾„
        # å°† YYYY-MM-DD æ ¼å¼è½¬æ¢ä¸º YYYY_MM_DD æ ¼å¼ï¼ˆæ–‡ä»¶å¤¹å‘½åæ ¼å¼ï¼‰
        date_folder = args.source_date.replace('-', '_')
        data_dir = f'datasets/OpenRCA/Bank/telemetry/{date_folder}'
        print(f"  ğŸ“ æ•°æ®ç›®å½•: {data_dir} (æ ¹æ® --source-date è‡ªåŠ¨æ„å»º)")
    else:
        # ä½¿ç”¨æŒ‡å®šçš„ data-dir
        data_dir = args.data_dir
        print(f"  ğŸ“ æ•°æ®ç›®å½•: {data_dir}")
    
    # è®¡ç®—æ—¶é—´åç§»é‡
    try:
        # è§£ææºæ—¥æœŸå’Œç›®æ ‡æ—¥æœŸ
        source_date = datetime.strptime(args.source_date, '%Y-%m-%d')
        target_date = datetime.strptime(args.target_date, '%Y-%m-%d')
        
        # éªŒè¯ç›®æ ‡æ—¥æœŸå¿…é¡»åœ¨å½“å‰æ—¶é—´15å¤©å†…
        now = datetime.now(timezone.utc)
        target_datetime = target_date.replace(tzinfo=timezone.utc)
        
        if target_datetime > now:
            print(f"âŒ é”™è¯¯: ç›®æ ‡æ—¥æœŸ {args.target_date} ä¸èƒ½æ™šäºå½“å‰æ—¶é—´")
            return
        
        days_diff = (now - target_datetime).days
        if days_diff > 15:
            print(f"âŒ é”™è¯¯: ç›®æ ‡æ—¥æœŸ {args.target_date} å¿…é¡»åœ¨å½“å‰æ—¶é—´15å¤©å†…")
            print(f"   å½“å‰æ—¶é—´: {now.strftime('%Y-%m-%d %H:%M:%S')} UTC")
            print(f"   ç›®æ ‡æ—¥æœŸè·ç¦»å½“å‰æ—¶é—´: {days_diff} å¤©")
            return
        
        # è®¡ç®—åç§»é‡ï¼ˆæ¯«ç§’ï¼‰
        # å°†æºæ—¥æœŸçš„ 0 ç‚¹æ˜ å°„åˆ°ç›®æ ‡æ—¥æœŸçš„ 0 ç‚¹
        source_timestamp_ms = int(source_date.replace(tzinfo=timezone.utc).timestamp() * 1000)
        target_timestamp_ms = int(target_date.replace(tzinfo=timezone.utc).timestamp() * 1000)
        time_offset_ms = target_timestamp_ms - source_timestamp_ms
        config.time_offset_ms = time_offset_ms
        
        print(f"  â° æ—¶é—´æ˜ å°„:")
        print(f"     æºæ—¥æœŸ: {args.source_date} 00:00:00 UTC")
        print(f"     ç›®æ ‡æ—¥æœŸ: {args.target_date} 00:00:00 UTC")
        print(f"     æ—¶é—´åç§»: {time_offset_ms / (24*60*60*1000):.1f} å¤©")
        print()
        
    except ValueError as e:
        print(f"âŒ æ—¥æœŸæ ¼å¼é”™è¯¯: {e}")
        print("   æ—¥æœŸæ ¼å¼åº”ä¸º: YYYY-MM-DD (ä¾‹å¦‚: 2021-03-04)")
        return
    
    # æ£€æŸ¥æ•°æ®ç›®å½•
    data_dir_path = Path(data_dir)
    
    if not data_dir_path.exists():
        print(f"âŒ é”™è¯¯: æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        print(f"   è¯·æ£€æŸ¥ --source-date å‚æ•°æ˜¯å¦æ­£ç¡®ï¼ˆä¾‹å¦‚: 2021-03-04 å¯¹åº” 2021_03_04 æ–‡ä»¶å¤¹ï¼‰")
        return
    
    if not data_dir_path.is_dir():
        print(f"âŒ é”™è¯¯: æŒ‡å®šçš„è·¯å¾„ä¸æ˜¯ç›®å½•: {data_dir}")
        return
    
    start_time = time.time()
    time_range_info = {'min': None, 'max': None}  # è·Ÿè¸ªæ—¶é—´èŒƒå›´
    
    # ========== ç¬¬ä¸€æ­¥ï¼šå¯¼å…¥ Metric æ•°æ® ==========
    print("=" * 60)
    print("ğŸ“Š ç¬¬ä¸€æ­¥ï¼šå¯¼å…¥ Metric æ•°æ®")
    print("=" * 60)
    print()
    
    # è®¾ç½® Metric Exporter
    print("ğŸ”§ åˆå§‹åŒ– OpenTelemetry Metric Exporter...")
    try:
        metric_exporter = exporter_manager.get_metric_exporter()
        print("âœ… Metric Exporter åˆå§‹åŒ–å®Œæˆ\n")
    except Exception as e:
        print(f"âŒ Metric Exporter åˆå§‹åŒ–å¤±è´¥: {e}")
        logger.exception("Metric Exporter åˆå§‹åŒ–å¤±è´¥")
        return
    
    # æŸ¥æ‰¾ metric æ–‡ä»¶
    metric_app_files = sorted(glob.glob(str(data_dir_path / "**/metric_app.csv"), recursive=True))
    metric_container_files = sorted(glob.glob(str(data_dir_path / "**/metric_container.csv"), recursive=True))
    
    total_metrics = 0
    
    # å¤„ç† metric_app.csv æ–‡ä»¶
    if metric_app_files:
        print(f"ğŸ“‹ æ‰¾åˆ° {len(metric_app_files)} ä¸ª metric_app.csv æ–‡ä»¶\n")
        with tqdm(total=len(metric_app_files), desc="ğŸ“Š å¤„ç† metric_app æ–‡ä»¶", unit="æ–‡ä»¶", ncols=100) as file_pbar:
            for file_idx, metric_file in enumerate(metric_app_files, 1):
                file_name = Path(metric_file).name
                file_pbar.set_description(f"ğŸ“„ [{file_idx}/{len(metric_app_files)}] {file_name}")
                
                file_count = process_metric_app_file(
                    metric_file,
                    metric_exporter,
                    config.batch_size,
                    time_range_info=time_range_info,
                    time_offset_ms=config.time_offset_ms,
                    show_progress=True,
                    resource_manager=resource_manager
                )
                
                total_metrics += file_count
                file_pbar.update(1)
                file_pbar.set_postfix({"metrics": f"{file_count:,}", "æ€»è®¡": f"{total_metrics:,}"})
        print()
    else:
        print("âš ï¸  æœªæ‰¾åˆ° metric_app.csv æ–‡ä»¶\n")
    
    # å¤„ç† metric_container.csv æ–‡ä»¶
    if metric_container_files:
        print(f"ğŸ“‹ æ‰¾åˆ° {len(metric_container_files)} ä¸ª metric_container.csv æ–‡ä»¶")
        print("   ğŸ’¡ ä½¿ç”¨æŒ‰å¤§ç±»åˆ†ç»„metricæ¨¡å¼: container metricå°†æŒ‰å¤§ç±»åˆ†ç»„ä¸º6ä¸ªmetricï¼š")
        print("      - container_mysql_metric (MySQLç›¸å…³)")
        print("      - container_oslinux_metric (æ“ä½œç³»ç»ŸLinuxç›¸å…³)")
        print("      - container_redis_metric (Redisç›¸å…³)")
        print("      - container_container_metric (Dockerå®¹å™¨ç›¸å…³)")
        print("      - container_tomcat_metric (Tomcatç›¸å…³)")
        print("      - container_jvm_metric (JVMç›¸å…³)")
        print("      è¿™æ ·å¯ä»¥å¤§å¤§å‡å°‘metricæ•°é‡ï¼ˆä»349ä¸ªå‡å°‘åˆ°6ä¸ªï¼‰ï¼Œä¾¿äºåœ¨SigNozä¸­ç®¡ç†å’ŒæŸ¥è¯¢\n")
        with tqdm(total=len(metric_container_files), desc="ğŸ“Š å¤„ç† metric_container æ–‡ä»¶", unit="æ–‡ä»¶", ncols=100) as file_pbar:
            for file_idx, metric_file in enumerate(metric_container_files, 1):
                file_name = Path(metric_file).name
                file_pbar.set_description(f"ğŸ“„ [{file_idx}/{len(metric_container_files)}] {file_name}")
                
                file_count = process_metric_container_file(
                    metric_file,
                    metric_exporter,
                    config.batch_size,
                    time_range_info=time_range_info,
                    time_offset_ms=config.time_offset_ms,
                    show_progress=True,
                    resource_manager=resource_manager
                )
                
                total_metrics += file_count
                file_pbar.update(1)
                file_pbar.set_postfix({"metrics": f"{file_count:,}", "æ€»è®¡": f"{total_metrics:,}"})
        print()
    else:
        print("âš ï¸  æœªæ‰¾åˆ° metric_container.csv æ–‡ä»¶\n")
    
    # ç­‰å¾… metric æ•°æ®å‘é€å®Œæˆï¼ˆå°†åœ¨æœ€åç»Ÿä¸€å…³é—­æ‰€æœ‰ exportersï¼‰
    
    # ========== ç¬¬äºŒæ­¥ï¼šå¯¼å…¥ Trace æ•°æ® ==========
    print("=" * 60)
    print("ğŸ” ç¬¬äºŒæ­¥ï¼šå¯¼å…¥ Trace æ•°æ®")
    print("=" * 60)
    print()
    
    # è®¾ç½® Trace Exporter
    print("ğŸ”§ åˆå§‹åŒ– OpenTelemetry Trace Exporter...")
    try:
        trace_exporter = exporter_manager.get_trace_exporter()
        print("âœ… Trace Exporter åˆå§‹åŒ–å®Œæˆ\n")
    except Exception as e:
        print(f"âŒ Trace Exporter åˆå§‹åŒ–å¤±è´¥: {e}")
        logger.exception("Trace Exporter åˆå§‹åŒ–å¤±è´¥")
        return
    
    # æŸ¥æ‰¾ trace æ–‡ä»¶
    trace_files = sorted(glob.glob(str(data_dir_path / "**/trace_span.csv"), recursive=True))
    
    if not trace_files:
        print(f"âš ï¸  æœªæ‰¾åˆ° trace_span.csv æ–‡ä»¶åœ¨ç›®å½•: {data_dir}")
        print(f"   è¯·æ£€æŸ¥:")
        print(f"   1. ç›®å½•ç»“æ„æ˜¯å¦æ­£ç¡®ï¼ˆåº”åŒ…å« trace/trace_span.csv æ–‡ä»¶ï¼‰")
        print(f"   2. --source-date å‚æ•°æ˜¯å¦ä¸æ–‡ä»¶å¤¹åç§°åŒ¹é…ï¼ˆä¾‹å¦‚: 2021-03-04 å¯¹åº” 2021_03_04 æ–‡ä»¶å¤¹ï¼‰")
        print()
    else:
        print(f"ğŸ“‹ æ‰¾åˆ° {len(trace_files)} ä¸ª trace æ–‡ä»¶\n")
        
        # æµå¼å¤„ç†æ¯ä¸ªæ–‡ä»¶
        total_spans = 0
        
        # ä½¿ç”¨ tqdm æ˜¾ç¤ºæ•´ä½“è¿›åº¦
        with tqdm(total=len(trace_files), desc="ğŸ” å¤„ç† trace æ–‡ä»¶", unit="æ–‡ä»¶", ncols=100) as file_pbar:
            for file_idx, trace_file in enumerate(trace_files, 1):
                file_name = Path(trace_file).name
                file_pbar.set_description(f"ğŸ“„ [{file_idx}/{len(trace_files)}] {file_name}")
                
                file_count = process_file_streaming(
                    trace_file,
                    trace_exporter,
                    config.batch_size,
                    time_range_info=time_range_info,
                    time_offset_ms=config.time_offset_ms,
                    show_progress=True,
                    resource_manager=resource_manager
                )
                
                total_spans += file_count
                file_pbar.update(1)
                file_pbar.set_postfix({"spans": f"{file_count:,}", "æ€»è®¡": f"{total_spans:,}"})
        
        print()  # ç©ºè¡Œåˆ†éš”
        
        # ç¡®ä¿æ‰€æœ‰ trace æ•°æ®éƒ½å‘é€å®Œæˆï¼ˆå°†åœ¨æœ€åç»Ÿä¸€å…³é—­æ‰€æœ‰ exportersï¼‰
    
    # ç»Ÿä¸€å…³é—­æ‰€æœ‰ exporters
    print("â³ ç­‰å¾…æ‰€æœ‰æ•°æ®å‘é€å®Œæˆ...")
    exporter_manager.shutdown_all()
    print()
    
    print("ğŸ’¡ æç¤º: æ£€æŸ¥ SigNoz Collector æ—¥å¿—:")
    print(f"  docker logs <signoz-otel-collector-container> --tail 50")
    print("  å¦‚æœçœ‹åˆ° metric å’Œ trace ç›¸å…³çš„æ—¥å¿—ï¼Œè¯´æ˜æ•°æ®å·²æˆåŠŸæ¥æ”¶")
    
    elapsed_time = time.time() - start_time
    print()
    print("=" * 60)
    print("âœ… å¯¼å…¥å®Œæˆ!")
    if total_metrics > 0:
        print(f"  ğŸ“Š Metric è®°å½•æ•°: {total_metrics:,}")
    if trace_files:
        print(f"  ğŸ” Trace spans æ•°: {total_spans:,}")
    print(f"  â±ï¸  æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
    if total_metrics > 0:
        print(f"  ğŸ“ˆ Metric å¹³å‡é€Ÿåº¦: {total_metrics / elapsed_time:.0f} metrics/ç§’")
    if trace_files and total_spans > 0:
        print(f"  ğŸ“ˆ Trace å¹³å‡é€Ÿåº¦: {total_spans / elapsed_time:.0f} spans/ç§’")
    
    # æ¸…ç†èµ„æº
    resource_manager.clear_cache()
    
    # æ˜¾ç¤ºæ—¶é—´èŒƒå›´ä¿¡æ¯
    if time_range_info['min'] is not None and time_range_info['max'] is not None:
        # ä½¿ç”¨ UTC æ—¶é—´æ˜¾ç¤ºï¼ˆOpenTelemetry å’Œ SigNoz ä½¿ç”¨ UTCï¼‰
        min_dt_utc = datetime.utcfromtimestamp(time_range_info['min'] / 1000)
        max_dt_utc = datetime.utcfromtimestamp(time_range_info['max'] / 1000)
        min_dt_local = datetime.fromtimestamp(time_range_info['min'] / 1000)
        max_dt_local = datetime.fromtimestamp(time_range_info['max'] / 1000)
        
        print()
        print("ğŸ“… æ•°æ®æ—¶é—´èŒƒå›´:")
        print(f"  â° æœ€æ—©æ—¶é—´ (UTC): {min_dt_utc.strftime('%Y-%m-%d %H:%M:%S')} UTC")
        print(f"  â° æœ€æ™šæ—¶é—´ (UTC): {max_dt_utc.strftime('%Y-%m-%d %H:%M:%S')} UTC")
        if min_dt_local != min_dt_utc:
            print(f"  â° æœ€æ—©æ—¶é—´ (æœ¬åœ°): {min_dt_local.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"  â° æœ€æ™šæ—¶é—´ (æœ¬åœ°): {max_dt_local.strftime('%Y-%m-%d %H:%M:%S')}")
        print()
        print("âš ï¸  é‡è¦æç¤º: SigNoz ä½¿ç”¨ UTC æ—¶é—´ï¼Œè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤è®¾ç½®:")
        # è®¡ç®—å¼€å§‹å’Œç»“æŸçš„æ—¥æœŸï¼ˆUTCï¼‰ï¼ŒåŒ…å«å‰åå„ä¸€å¤©ä»¥ç¡®ä¿è¦†ç›–
        start_date = min_dt_utc.strftime('%Y-%m-%d')
        end_date = max_dt_utc.strftime('%Y-%m-%d')
        
        # å¦‚æœæ•°æ®è·¨è¶Šå¤šå¤©ï¼Œä½¿ç”¨æ›´å®½çš„æ—¶é—´èŒƒå›´
        start_date_obj = datetime.strptime(start_date, '%Y-%m-%d')
        end_date_obj = datetime.strptime(end_date, '%Y-%m-%d')
        # æ‰©å±•ä¸€å¤©ä»¥ç¡®ä¿è¦†ç›–
        extended_start = (start_date_obj - timedelta(days=1)).strftime('%Y-%m-%d')
        extended_end = (end_date_obj + timedelta(days=1)).strftime('%Y-%m-%d')
        
        print(f"  1. ç‚¹å‡» SigNoz å³ä¸Šè§’çš„æ—¶é—´é€‰æ‹©å™¨")
        print(f"  2. é€‰æ‹© 'Custom Time Range' æˆ– 'è‡ªå®šä¹‰æ—¶é—´èŒƒå›´'")
        print(f"  3. è®¾ç½®å¼€å§‹æ—¶é—´: {extended_start} 00:00:00")
        print(f"  4. è®¾ç½®ç»“æŸæ—¶é—´: {extended_end} 23:59:59")
        print(f"  5. æ—¶åŒºé€‰æ‹©: UTC (é‡è¦ï¼)")
        print(f"  6. ç‚¹å‡»åº”ç”¨")
        print()
        print(f"  ğŸ’¡ æ•°æ®å®é™…èŒƒå›´: {start_date} ~ {end_date} (UTC)")
        print(f"  ğŸ’¡ å»ºè®®æŸ¥è¯¢èŒƒå›´: {extended_start} ~ {extended_end} (UTC)")
        print()
        print("ğŸ” å¦‚æœè¿˜æ˜¯çœ‹ä¸åˆ°æ•°æ®ï¼Œè¯·æ£€æŸ¥:")
        print(f"  â€¢ æœåŠ¡åç§°è¿‡æ»¤å™¨: é€‰æ‹© '{args.service_name}' æˆ–æ¸…é™¤æ‰€æœ‰è¿‡æ»¤å™¨")
        print("  â€¢ ç­‰å¾… 2-5 åˆ†é’Ÿè®©æ•°æ®ç´¢å¼•å®Œæˆ")
        print("  â€¢ åˆ·æ–°æµè§ˆå™¨é¡µé¢")
        print("  â€¢ æŸ¥çœ‹ SigNoz æ—¥å¿—: docker logs <signoz-container>")
    
    print("=" * 60)


if __name__ == '__main__':
    main()

